---
title: "Nearly Optimal State Merklization (in Polkadot-SDK)"
layout: post
excerpt: "Building upon ideas for a novel Merkle Trie database"
twitter-image: TODO
---

Recently, my friend and coworker [Sergei (Pepyakin)](https://pep.wtf) sent me an article from Preston Evans on the subject of a more optimal Merkle Trie format designed to be efficient on SSDs. The original article is [here](https://www.prestonevans.me/nearly-optimal-state-merklization/) and I highly recommend it as background reading to this post.

TODO: twitter card https://twitter.com/sovereign_labs/status/1744768837982011472

The optimizations presented in the original post sparked a two-day conversation with Pep in which we discussed how this might be made to work with the [Polkadot-SDK](https://github.com/paritytech/polkadot-sdk) as well. Polkadot-SDK, while it also uses a Merkle Trie to store state, was designed on a differing set of assumptions, and so the original approach would need to be adapted in order to be integrated. This post might be seen as a summary of our conversation, covering some history, some of the original optimizations, the differences in assumptions, and tweaks that may be made in order to maintain full backwards compatibility with the Polkadot-SDK. Some familiarity with [Merkle Tries](https://en.wikipedia.org/wiki/Merkle_tree), especially as they are used as blockchain state databases, will help in comprehending this article, but all are welcome to come along for the ride.

Preston's proposed system, in a nutshell, is a new binary Merkle Trie format and database schema that is extremely low-overhead and amenable to SSDs with most (if not all) disk accesses being predictable with no other information beyond the key being queried. We'll revisit more specifics later, though I highly recommend reading the original blog post for a high-fidelity explanation. 

---

But first, let's cover _why_ optimization of the Merkle Trie is so important. The open secret about scaling serial blockchain execution is that most of the time isn't actually spent on executing transactions: it's on reading and writing state data to and from disk, such as accounts, balances, and smart contract state. Merkle Tries in blockchains are not strictly necessary, but they provide a means of easily proving some of the state of the ledger to a light client. They are a key part of permissionless blockchain systems which seek to include light clients, not just full nodes, as full participants.

Traversing a Merkle Trie to load some particular piece of state is a log(n) operation in the number of items within the trie. Updates to the Merkle Trie are logarithmic as well. The implication is that a larger state, even if the majority of this state is dormant, makes reading and writing more expensive. This is the real reason why Ethereum hasn't "just increased the gas limit" and why Polkadot has required deposits for all stored state: state growth is a huge problem, adds bloat to the Merkle Trie, and slows everything else down as a result. What makes Preston's article so important is that it shows a way for us to maintain the wonderful advantages of merklized state while drastically reducing the overheads associated with previous implementations.

When it comes to the Polkadot-SDK, I see this design being far more useful to Parachains than the Polkadot Relay Chain itself. The Relay Chain has relatively little state, having offloaded most of its work onto System Parachains. For parachains, the benefits will come for two reasons. Reason one is that it's more data-efficient, utilizing less of what we refer to in Polkadot-land as the Proof-of-Validity. Reason two is that (at the time of this writing), work on [Elastic Scaling](https://github.com/paritytech/polkadot-sdk/issues/1829) is underway, and it will in theory bound the throughput of a parachain at the rate a single node is capable of processing transactions. I foresee a future for some parachains where the Merkle Trie will be a bottleneck.

---

In 2016, the first significant project I worked on in the blockchain space was optimizing the Parity-Ethereum node's implementation of Ethereum's Merkle-Patricia Trie. At that time, blockchain node technology was a lot less sophisticated. We were in a friendly rivalry with the geth team, and one way we wanted to get a leg up on geth's performance was by implementing batch writes, where we'd only compute all of the changes in the Merkle-Patricia trie once at the end of the block (actually, the end of each transaction - at that time, transaction receipts all carried an intermediate state root). The status quo was to rather apply updates to trie nodes individually as state changes occurred during transaction execution. This may be hard to believe for node developers of today, but hey, it was 2016, and it worked - and it gave our Ethereum nodes a significant boost in performance.

We've come a long way since 2016, but many of the inefficiencies of the 16-radix Merkle-Patricia trie used in Ethereum and the Polkadot-SDK still persist. They have minor differences in node encoding and formats, but function much the same. The radix of 16 was chosen because it reduced one of the biggest problems with traversing Merkle Tries: random accesses. Child nodes are referenced by their hash, and these hashes are randomly distributed. If your traversal algorithm is naive and you load each child node as you learn its hash, you end up breaking one of the first laws of computer program optimization, which is to maintain data locality. What's even worse is breaking that law in disk access patterns. State Merkle Tries in Ethereum, Polkadot-SDK, and countless other protocols still work this way today.

One of the other issues with the 16-radix Merkle-Patricia trie is that it's very space inefficient. Beyond the first few layers, the state trie gets pretty sparse, and most of the 16 slots for children at each branch node are not occupied. All else equal, the 16-trie is more efficient than its binary counterpart in the number of disk accesses that need to be made. However, in a world of light and stateless blockchain nodes, where Merkle proofs need to be submitted over the network, sending all these mostly-empty nodes wastes space.

There have been advancements, such as the [Jellyfish Merkle Trie](https://developers.diem.com/papers/jellyfish-merkle-tree/2021-01-14.pdf?ref=127.0.0.1) pioneered at Diem. To summarize briefly - Jellyfish is pretty clever, and allows the same trie to be represented either in a binary format (over the network), or in a 16-radix format (on disk). It has other optimizations which aim to replace common sequence patterns of nodes with very efficient representations to minimize the required steps in traversal, update, and proving. They also remark on an approach to storing trie nodes on disk which avoids as much write amplification (read: overhead) in a RocksDB-based implementation. Jellyfish is definitely an improvement, but it also makes a key trade-off: the assumption that the keys used in the state trie have a fixed length.

We'll address the subject of fixed-length keys in more detail later, but it's the root of the differences between Preston's assumptions and the ones we'll be working with in this post. This difference in some sense is the real subject of this article. The Polkadot-SDK has taken an alternative path down the "tech tree" stemming from the Merkle-Patricia Trie of Ethereum. While Ethereum only ever uses fixed-length keys in its state trie, the underlying data structure actually supports arbitrary-length keys by virtue of the branch node carrying an optional value. Polkadot-SDK takes full advantage of this property and may actually be the only system to do so.

---

It's now time to visit the optimizations and differing assumptions, and modifications that might be made to apply these same optimizations (in spirit) to the Polkadot-SDK.

Preston's post makes a few implicit assumptions which I would like to make explicit here.

1. **Keys have a fixed length**. As mentioned above, this is a big one. 
1. **Stored keys are close to uniformly distributed across the key space**. This is also important, as it implies that the state trie has a roughly uniform depth at all points and therefore guesses about how long a traversal will be are likely to be accurate across the whole state trie.
1. **Only one version of the trie needs to be stored on disk**. I view this assumption as an implication of Proof-of-Stake, where fast (if not single-slot) finality has become the norm. In particular, this assumption makes sense for Sovereign - they target Celestia as one of their main platforms, and Celestia has single-slot finality.

Polkadot-SDK has made different decisions - in some cases, slightly different, in others, wildly different. Respectively:

1. **Keys do not have a fixed length**. The storage API exposed to the business logic of a Polkadot-SDK chain is a simple mapping from arbitrary byte-strings to arbitrary byte-strings. Keys are not meant to be hashes, though they are allowed to be.
2. **Keys often have long shared prefixes**. Chains built with the Polkadot-SDK are comprised of **modules**. All storage from any particular module has a shared prefix - and all storage from any particular map within a particular module also has a shared prefix. The set of these shared prefixes is relatively small. More on why in a moment.
3. **Only one version of the trie needs to be stored on disk, but finality is not instant**. This is a small difference and it's fairly trivially addressable with in-memory overlays for any unfinalized blocks, but does need to be handled.

Keys not having a fixed length and having long shared prefixes is a huge difference! There is a good reason for this: it enables the tree to be _iterable_. In Polkadot-SDK, you can iterate the entire storage for a module, or for a particular mapping within a module. This is a key part of what enables Polkadot-SDK's to support trustless upgrades: you can deploy migrations that automatically iterate and migrate entire swathes of storage without needing anyone to freeze the chain, generate a list of keys to alter or delete, and so on. State systems where the keys for related storage entries are dispersed throughout a forest of other, unrelated keys cannot provide any automated migration mechanisms. This is a very intentional product decision, but it makes Preston's proposal incompatible in a couple ways.

Without going into the details of the original approach yet (and I'd encourage having that article open as a reference), I'll sketch the alterations that need to be made in order to support the differences in assumptions around keys.

The first incompatibility is that nodes need to be _extremely compact_ and have _uniform size_. The outcome of fixed-length keys is that all values are logically at the same depth in the merkle tree. Even though the last parts of the path will be compressed, there is no possibility for a merkle tree node to have a left child, a right child, _or_ a value. Nodes are either branches or leaves in Preston's proposal, and the rest of the optimizations hinge on that. We will address this by giving each key a **uniform and logically large size** and then implementing an **efficient padding mechanism** that eliminates overhead from these logically large keys.
    
    - sketch: keys are actually all logically `key ++ len(key) ++ remaining zero bytes` where len(key) is u16 or u32. leaf nodes just contain the original key.

The second incompatibility is that long shared prefixes means long traversals, assuming that your only two kinds of nodes are leaf nodes and branch nodes. Many Polkadot-SDK keys start with 256-bit shared prefixes - that means you'd traverse through 256 layers of the binary trie before even getting to a differentiated part. Luckily, Ethereum and Polkadot-SDK already worked around this issue with an approach known as **extension nodes**, which encodes a long shared run of bits in keys, and are followed by a branch. These work slightly differently in Ethereum and Polkadot-SDK, but achieve the same effect. Systems like Jellyfish have gotten rid of them entirely, because if your keys are uniformly distributed the odds of having long shared prefixes in a crowded trie are pretty small. But in the Polkadot-SDK model they still make sense. The solution is to **introduce extension nodes without substantially increasing disk accesses**. We'll also use these extension nodes in our solution to the first problem.
    
    - sketch: steal an extra bit for domain separation, put extension node children in their proper page, smarter pre-fetching based on omitted pages for common prefixes, cache for preimages of extensions